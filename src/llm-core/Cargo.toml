[package]
name = "llm-core"
version.workspace = true
edition.workspace = true
authors.workspace = true
license.workspace = true
description = "LLM Core Service - Local LLM inference with persona support"

[dependencies]
tokio = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
anyhow = { workspace = true }
thiserror = { workspace = true }
tracing = { workspace = true }
tracing-subscriber = { workspace = true }
uuid = { workspace = true }
chrono = { workspace = true }
axum = { workspace = true }
tower = { workspace = true }
tower-http = { workspace = true }
config = { workspace = true }
clap = { workspace = true }
reqwest = { workspace = true }
fastrand = "2.0"
# LLM inference - GGUF support will be added via FFI or external library
# NOTE: llama-cpp-rs doesn't exist, will use llama.cpp via FFI or wait for stable crate
# Using Synap for LLM inference instead
tokenizers = { workspace = true }
# NOTE: semantic-compressor temporarily disabled - will be enabled when needed
# semantic-compressor = { workspace = true }

[features]
default = []
# gguf feature will be enabled when llama.cpp integration is ready
# gguf = []
# s2s: Enable server-to-server tests (requires active Synap server)
s2s = []

[dev-dependencies]
tokio-test = { workspace = true }
mockall = { workspace = true }



