# Estrat√©gias de Otimiza√ß√£o de Lat√™ncia - XTTS Streaming

**Data**: 2025-11-29  
**Status**: ‚úÖ Implementadas / ‚ö†Ô∏è Em otimiza√ß√£o

---

## üìä Problema Atual

- **Lat√™ncia inicial**: 2.5-3.0s (target: ‚â§ 0.8s para FAST)
- **Primeiro chunk**: 39 chars ‚Üí 2.5-3.0s de √°udio (RTF ~1.2-1.3x)
- **Repeti√ß√£o entre chunks**: "otherworldly light" repetido (problema de overlap)

---

## ‚úÖ Estrat√©gias Implementadas

### 1. Chunker Inteligente com Pontua√ß√£o

**Status**: ‚úÖ Implementado

- **Primeiro chunk**: Vai at√© primeira v√≠rgula/ponto (nunca corta no meio)
- **Chunks subsequentes**: Sempre procura pontua√ß√£o antes de cortar
- **Ap√≥s 5s de √°udio**: Respeita limites de frase (natural pauses)
- **Busca estendida**: Procura pontua√ß√£o at√© 2x o limite se necess√°rio
- **Warning**: Alerta se chunk for finalizado sem pontua√ß√£o

**C√≥digo**: `split_text_for_tts()` com l√≥gica de busca de pontua√ß√£o

### 2. FP16 (Half Precision)

**Status**: ‚úÖ Implementado (parcialmente)

- **Autocast**: `torch.cuda.amp.autocast()` durante infer√™ncia
- **Model conversion**: Tentativa de converter modelo para `.half().cuda()`
- **Problema**: Pode n√£o estar totalmente ativo (verificar)

**C√≥digo**: `synthesize_with_profile()` com `use_fp16=True`

### 3. Warm-up na Inicializa√ß√£o

**Status**: ‚úÖ Implementado

- **Execu√ß√£o**: Uma infer√™ncia curta ao carregar modelo
- **Objetivo**: "Compilar" kernels CUDA antes do primeiro uso real
- **Tempo**: ~2-3s (aceit√°vel, s√≥ acontece uma vez)

**C√≥digo**: `main()` - warm-up ap√≥s carregar modelo

### 4. Pre-buffer Reduzido

**Status**: ‚úÖ Implementado

- **FAST profile**: 200ms (reduzido de 240ms)
- **CINEMATIC profile**: 500ms
- **Objetivo**: Come√ßar playback mais r√°pido

**C√≥digo**: `TtsProfile.fast()` - `initial_prebuffer_ms: 200`

### 5. Blocos de √Åudio Menores

**Status**: ‚úÖ Implementado

- **FAST profile**: 25ms (reduzido de 50ms)
- **CINEMATIC profile**: 60ms
- **Objetivo**: Streaming mais fino, menor lat√™ncia percebida

**C√≥digo**: `TtsProfile.fast()` - `audio_block_ms: 25`

### 6. Primeiro Chunk Otimizado

**Status**: ‚úÖ Implementado

- **Tamanho**: 20 chars (reduzido de 30)
- **Estrat√©gia**: Vai at√© primeira v√≠rgula (39 chars no exemplo)
- **Objetivo**: Gerar menos √°udio no primeiro chunk

**C√≥digo**: `TtsProfile.fast()` - `first_chunk_max_chars: 20`

### 7. Limpeza de Cache CUDA

**Status**: ‚úÖ Implementado

- **Entre chunks**: `torch.cuda.empty_cache()` ap√≥s cada chunk
- **Ap√≥s warm-up**: Limpeza e sincroniza√ß√£o
- **Objetivo**: Evitar ac√∫mulo de mem√≥ria que pode causar lentid√£o

**C√≥digo**: Ap√≥s cada `synthesize_with_profile()`

---

## ‚ö†Ô∏è Problemas Identificados

### 1. Repeti√ß√£o entre Chunks

**Sintoma**: "otherworldly light" repetido no final/in√≠cio de chunks

**Causa poss√≠vel**:
- Overlap na concatena√ß√£o de blocos
- Chunks sendo duplicados
- Problema na l√≥gica de split de blocos

**Solu√ß√£o proposta**:
- Verificar se h√° overlap na concatena√ß√£o
- Garantir que blocos n√£o se sobreponham
- Validar que cada chunk √© √∫nico

### 2. FP16 N√£o Totalmente Ativo

**Sintoma**: RTF ainda acima de 1.0x (1.2-1.3x)

**Causa poss√≠vel**:
- Modelo n√£o est√° em half precision
- Autocast n√£o est√° sendo aplicado corretamente
- XTTS pode n√£o suportar FP16 completamente

**Solu√ß√£o proposta**:
- Verificar se modelo est√° realmente em FP16
- Usar `torch.compile()` para otimizar
- Verificar se XTTS suporta FP16 nativamente

### 3. Primeiro Chunk Ainda Gera Muito √Åudio

**Sintoma**: 39 chars geram 2.5-3.0s de √°udio (deveria ser ~1s)

**Causa poss√≠vel**:
- XTTS adiciona pausas/padding
- Modelo n√£o est√° otimizado
- Sample rate ou configura√ß√£o incorreta

**Solu√ß√£o proposta**:
- Verificar configura√ß√£o de sample rate
- Reduzir ainda mais primeiro chunk (10-15 chars)
- Usar texto mais curto para primeiro chunk

---

## üöÄ Estrat√©gias Adicionais Propostas

### 1. Torch Compile (JIT Compilation)

**Objetivo**: Compilar modelo para acelerar primeira infer√™ncia

```python
# Compilar modelo ap√≥s warm-up
if hasattr(tts.synthesizer, 'model'):
    tts.synthesizer.model = torch.compile(tts.synthesizer.model, mode="reduce-overhead")
```

**Benef√≠cio esperado**: 20-30% redu√ß√£o na primeira infer√™ncia

### 2. Pre-load Speaker Embedding

**Objetivo**: Cachear embedding do speaker antes do primeiro uso

```python
# Pre-load speaker WAV embedding
speaker_wav_path = "path/to/speaker.wav"
tts.synthesizer.speaker_manager.compute_speaker_embedding(speaker_wav_path)
```

**Benef√≠cio esperado**: 100-200ms redu√ß√£o na primeira infer√™ncia

### 3. CUDA Streams Paralelos

**Objetivo**: Usar m√∫ltiplos CUDA streams para paralelizar

```python
# Criar streams separados para diferentes opera√ß√µes
stream1 = torch.cuda.Stream()
stream2 = torch.cuda.Stream()
```

**Benef√≠cio esperado**: Redu√ß√£o de lat√™ncia em GPUs high-end

### 4. Model Quantization (INT8)

**Objetivo**: Quantizar modelo para INT8 (mais agressivo que FP16)

```python
# Quantizar modelo para INT8
model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
```

**Benef√≠cio esperado**: 40-50% redu√ß√£o de lat√™ncia (com poss√≠vel perda de qualidade)

### 5. Text Pre-processing Otimizado

**Objetivo**: Reduzir tamanho do texto antes de enviar para XTTS

- Remover espa√ßos extras
- Normalizar pontua√ß√£o
- Pr√©-processar para reduzir tokens

**Benef√≠cio esperado**: 10-20% redu√ß√£o de lat√™ncia

### 6. Streaming com Primeiro Bloco M√≠nimo

**Objetivo**: Come√ßar playback com apenas 1-2 blocos (50-100ms)

```python
# Reduzir pre-buffer para m√≠nimo absoluto
initial_prebuffer_ms = 100  # Apenas 2 blocos de 50ms
```

**Benef√≠cio esperado**: Redu√ß√£o de 100-150ms na lat√™ncia inicial

### 7. Model Caching e Pre-loading

**Objetivo**: Manter modelo sempre em mem√≥ria GPU

```python
# Manter modelo em GPU ap√≥s carregamento
model = model.cuda()
torch.cuda.empty_cache()  # Limpar apenas cache, n√£o modelo
```

**Benef√≠cio esperado**: Eliminar lat√™ncia de carregamento

### 8. Batch Processing Otimizado

**Objetivo**: Processar m√∫ltiplos chunks pequenos em batch

```python
# Agrupar chunks pequenos para processar juntos
small_chunks = [chunk1, chunk2, chunk3]
batch_audio = tts.batch_synthesize(small_chunks)
```

**Benef√≠cio esperado**: Melhor utiliza√ß√£o de GPU

---

## üìã Checklist de Verifica√ß√£o

### Chunker
- [x] Primeiro chunk vai at√© primeira v√≠rgula
- [x] Chunks subsequentes procuram pontua√ß√£o
- [x] Busca estendida se pontua√ß√£o n√£o encontrada pr√≥xima
- [ ] Valida√ß√£o de n√£o-overlap entre chunks
- [ ] Log de chunks para debug

### FP16
- [x] Autocast implementado
- [ ] Modelo realmente em half precision (verificar)
- [ ] Verificar se XTTS suporta FP16
- [ ] Medir ganho real de FP16

### Warm-up
- [x] Warm-up executado na inicializa√ß√£o
- [x] Warm-up usa FP16
- [ ] Warm-up com texto similar ao primeiro chunk real
- [ ] Verificar se warm-up est√° realmente compilando kernels

### Pre-buffer
- [x] Pre-buffer reduzido para 200ms (FAST)
- [ ] Testar com pre-buffer ainda menor (100ms)
- [ ] Validar que n√£o causa underrun

### Blocos de √Åudio
- [x] Blocos reduzidos para 25ms (FAST)
- [ ] Verificar se n√£o causa overhead de processamento
- [ ] Validar continuidade entre blocos

### Otimiza√ß√µes Adicionais
- [ ] Torch compile implementado
- [ ] Speaker embedding pr√©-carregado
- [ ] CUDA streams paralelos
- [ ] Model quantization (se necess√°rio)
- [ ] Text pre-processing
- [ ] Model caching otimizado

---

## üéØ Targets de Performance

### FAST Profile (Qwen 1.5B)
- **Target atual**: ‚â§ 0.8s `time_to_first_audio`
- **Atual**: 2.5-3.0s
- **Gap**: ~2.2s a reduzir

### Estrat√©gias para Reduzir 2.2s

1. **FP16 totalmente ativo**: -0.5s (estimado)
2. **Torch compile**: -0.3s (estimado)
3. **Pre-buffer m√≠nimo (100ms)**: -0.1s
4. **Primeiro chunk menor (10-15 chars)**: -0.5s
5. **Speaker embedding pr√©-carregado**: -0.2s
6. **Otimiza√ß√µes de modelo**: -0.6s

**Total estimado**: -2.2s ‚Üí **Target ating√≠vel**

---

## üîç Debugging

### Verificar Overlap/Repeti√ß√£o

```python
# Adicionar valida√ß√£o de chunks
for i, chunk in enumerate(chunks):
    if i > 0:
        # Verificar se h√° overlap com chunk anterior
        prev_chunk_end = chunks[i-1][-20:]  # √öltimas 20 chars
        current_chunk_start = chunk[:20]  # Primeiras 20 chars
        if prev_chunk_end in current_chunk_start or current_chunk_start in prev_chunk_end:
            print(f"‚ö†Ô∏è  OVERLAP detectado entre chunks {i-1} e {i}")
```

### Verificar FP16

```python
# Verificar se modelo est√° em FP16
if hasattr(tts.synthesizer, 'model'):
    model_dtype = next(tts.synthesizer.model.parameters()).dtype
    print(f"Model dtype: {model_dtype}")  # Deve ser torch.float16
```

### Medir Lat√™ncia por Componente

```python
# Medir cada etapa separadamente
text_prep_start = time.time()
# ... prepara√ß√£o de texto ...
text_prep_time = time.time() - text_prep_start

tts_start = time.time()
# ... s√≠ntese XTTS ...
tts_time = time.time() - tts_start

block_split_start = time.time()
# ... split em blocos ...
block_split_time = time.time() - block_split_start
```

---

## üìö Refer√™ncias

- [TTS_PROFILES_STRATEGY.md](../docs/TTS_PROFILES_STRATEGY.md) - Estrat√©gia de perfis
- [TTS_PROFILES_SPEC.md](../../rulebook/tasks/implement-tts-service/specs/tts-service/TTS_PROFILES_SPEC.md) - Especifica√ß√£o t√©cnica
- [ARCHITECTURE.md](../../docs/ARCHITECTURE.md) - Arquitetura do sistema

---

**√öltima atualiza√ß√£o**: 2025-11-29



