//! TTS Pipeline - XTTS-only pipeline with voice embeddings
//!
//! This module orchestrates the complete voice synthesis pipeline:
//! 1. Parse Voice INTENT from Qwen LLM output
//! 2. Synthesize audio with XTTS using character-specific embeddings
//! 3. Return final audio (RAW, no post-processing)

use crate::error::Result;
use crate::interjections::InterjectionManager;
use crate::metrics::{LatencyTimer, MetricsCollector, PipelineMetrics, SharedMetricsCollector};
use crate::tts_profile::TtsProfile;
use crate::voice_intent::VoiceIntentParser;
use crate::voice_profiles::VoiceProfileManager;
use crate::xtts::{SharedXttsModel, SynthesisRequest, XttsModel};
use std::path::Path;
use std::sync::Arc;
use std::time::Instant;
use tokio::sync::RwLock;
use tokio::time::{sleep, Duration};
use tracing::{info, warn};

#[derive(Debug, Clone)]
pub struct PipelineRequest {
    pub text: String,     // Text from Qwen (may contain VOICE tags)
    pub language: String,  // "pt" or "en"
    /// Timestamp when user finished speaking (for interjection delay calculation)
    pub user_speech_end_ts: Option<Instant>,
    /// LLM model name (for TTS profile selection and interjection threshold)
    pub llm_model_name: Option<String>,
}

#[derive(Debug, Clone)]
pub struct PipelineResponse {
    pub audio: Vec<f32>,
    pub sample_rate: u32,
    pub channels: u16,
    pub duration_ms: u64,
    pub actor: String,
    pub emotion: String,
    pub style: String,
    /// Interjection that was used (if any)
    pub interjection_used: Option<String>,
    /// Time from user speech end to interjection start (if used)
    pub time_to_interjection_ms: Option<u64>,
}

pub struct TtsPipeline {
    xtts_model: SharedXttsModel,
    voice_profiles: Arc<RwLock<VoiceProfileManager>>,
    metrics: SharedMetricsCollector,
    base_path: std::path::PathBuf,
    interjection_manager: Option<Arc<InterjectionManager>>,
}

impl TtsPipeline {
    pub fn new(base_path: &Path) -> Self {
        // Create XTTS model with Coqui XTTS enabled and GPU activated
        // Model will be loaded on first synthesis
        // GPU is essential for < 1.5s latency
        let use_gpu = std::env::var("VRPG_TTS_USE_GPU")
            .unwrap_or_else(|_| "true".to_string())
            .parse::<bool>()
            .unwrap_or(true);
        let xtts = XttsModel::new_with_options(true, use_gpu, None); // use_coqui_xtts=true, use_gpu=true

        // Initialize voice profile manager
        let voice_profiles = VoiceProfileManager::new(base_path);

        // Try to load interjection manager
        let interjection_manager = Self::load_interjection_manager(base_path);

        Self {
            xtts_model: Arc::new(RwLock::new(xtts)),
            voice_profiles: Arc::new(RwLock::new(voice_profiles)),
            metrics: Arc::new(RwLock::new(MetricsCollector::new())),
            base_path: base_path.to_path_buf(),
            interjection_manager,
        }
    }

    /// Load interjection manager from config file
    fn load_interjection_manager(base_path: &Path) -> Option<Arc<InterjectionManager>> {
        let config_path = base_path.join("config").join("interjections.yaml");
        if !config_path.exists() {
            warn!("Interjection config not found: {:?}", config_path);
            return None;
        }

        match InterjectionManager::from_config_file(&config_path, base_path.to_path_buf()) {
            Ok(manager) => {
                info!("Interjection manager loaded successfully");
                Some(Arc::new(manager))
            }
            Err(e) => {
                warn!("Failed to load interjection manager: {}", e);
                None
            }
        }
    }

    pub fn get_metrics(&self) -> SharedMetricsCollector {
        self.metrics.clone()
    }

    /// Initialize voice profiles (load embeddings)
    pub async fn initialize_voice_profiles(&self) -> Result<()> {
        let mut profiles = self.voice_profiles.write().await;
        profiles.load_default_profiles().await?;
        Ok(())
    }

    pub async fn synthesize(&self, request: PipelineRequest) -> Result<PipelineResponse> {
        // Step 1: Parse Voice INTENT from Qwen output
        let parsed = VoiceIntentParser::parse(&request.text)?;

        info!(
            "Parsed Voice INTENT: actor={}, emotion={}, style={}",
            parsed.voice_intent.actor, parsed.voice_intent.emotion, parsed.voice_intent.style
        );

        // Step 2: Get character ID and voice profile
        let character_id = Self::extract_character_id(&parsed.voice_intent.actor);
        
        // Get voice profile for character (contains embedding path)
        let voice_id = {
            let profiles = self.voice_profiles.read().await;
            if let Some(_profile) = profiles.get_profile(&character_id).await {
                // Use character-specific voice
                character_id.clone()
            } else {
                // Fallback to default voice
                warn!("Voice profile not found for '{}', using default", character_id);
                "dm".to_string()
            }
        };

        // Step 3: Synthesize audio with XTTS
        let total_timer = LatencyTimer::start();
        let synthesis_timer = LatencyTimer::start();
        info!("Starting XTTS synthesis for text: '{}'", parsed.text);
        info!("Text length: {} characters", parsed.text.len());
        info!("Using voice: {}", voice_id);

        let xtts_audio = {
            info!("üé§ Using XTTS for synthesis");
            let xtts = self.xtts_model.read().await;
            if !xtts.is_loaded() {
                drop(xtts);
                let mut xtts = self.xtts_model.write().await;
                let _ = xtts.load("dummy").await;
                drop(xtts);
            }
            let xtts = self.xtts_model.read().await;

            // Create request for XTTS
            let xtts_request = SynthesisRequest {
                text: parsed.text.clone(),
                voice_id: voice_id.clone(),
                speed: 1.0,
                pitch: 0.0,
            };

            xtts.synthesize(&xtts_request).await?
        };

        info!(
            "XTTS audio generated: {} samples, {} Hz",
            xtts_audio.samples.len(),
            xtts_audio.sample_rate
        );

        let synthesis_latency_ms = synthesis_timer.elapsed_ms();
        info!("Synthesis completed in {} ms", synthesis_latency_ms);

        // Step 4: Calculate duration and record metrics
        let duration_ms =
            (xtts_audio.samples.len() as f32 / xtts_audio.sample_rate as f32 * 1000.0) as u64;
        let total_latency_ms = total_timer.elapsed_ms();

        // Record metrics
        {
            let mut metrics = self.metrics.write().await;
            metrics.record_request(PipelineMetrics {
                total_latency_ms,
                xtts_latency_ms: synthesis_latency_ms,
                audio_duration_ms: duration_ms,
                cache_hit: false, // TODO: Track cache hits
            });
        }

        Ok(PipelineResponse {
            audio: xtts_audio.samples,
            sample_rate: xtts_audio.sample_rate,
            channels: xtts_audio.channels,
            duration_ms,
            actor: parsed.voice_intent.actor,
            emotion: parsed.voice_intent.emotion,
            style: parsed.voice_intent.style,
            interjection_used: None, // TODO: Implement interjection logic in synthesize
            time_to_interjection_ms: None,
        })
    }

    fn extract_character_id(actor: &str) -> String {
        // Extract character ID from actor string
        // Examples:
        // "NPC_Guard" -> "npc_guard"
        // "Wizard_Elder" -> "wizard_elder"
        // "mestre" -> "dm"
        // "dungeon_master" -> "dungeon_master_en"

        if actor == "mestre" || actor == "dm" {
            "dm".to_string()
        } else if actor == "dungeon_master" || actor == "dungeon_master_en" {
            "dungeon_master_en".to_string()
        } else {
            actor
                .to_lowercase()
                .replace("NPC_", "npc_")
                .replace("PLAYER_", "player_")
        }
    }

    pub async fn list_loaded_characters(&self) -> Vec<String> {
        let profiles = self.voice_profiles.read().await;
        profiles.list_profiles().await
            .iter()
            .map(|p| p.character_id.clone())
            .collect()
    }

    pub async fn is_xtts_loaded(&self) -> bool {
        self.xtts_model.read().await.is_loaded()
    }

    pub async fn get_character_info(
        &self,
        character_id: &str,
    ) -> Option<(Vec<String>, Vec<String>)> {
        let profiles = self.voice_profiles.read().await;
        if let Some(profile) = profiles.get_profile(character_id).await {
            // Return default emotions and styles
            // In the future, these could be profile-specific
            Some((
                vec!["neutral".to_string(), "happy".to_string(), "sad".to_string(), "angry".to_string()],
                vec!["normal".to_string(), "whisper".to_string(), "shout".to_string()],
            ))
        } else {
            None
        }
    }

    /// Get XTTS model (for streaming pipeline integration)
    pub fn get_xtts_model(&self) -> SharedXttsModel {
        self.xtts_model.clone()
    }

    /// Get voice profiles (for streaming pipeline integration)
    pub fn get_voice_profiles(&self) -> Arc<RwLock<VoiceProfileManager>> {
        self.voice_profiles.clone()
    }

    /// Get base path
    pub fn base_path(&self) -> &std::path::PathBuf {
        &self.base_path
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;

    #[tokio::test]
    async fn test_pipeline_synthesize() {
        let pipeline = TtsPipeline::new(&PathBuf::from("test_models"));

        // XTTS model is loaded automatically

        let request = PipelineRequest {
            text: r#"<VOICE actor="NPC_Guard" emotion="skeptic" style="dry">
"Emiss√°rio? De qual reino?"
</VOICE>"#
                .to_string(),
            language: "pt".to_string(),
            user_speech_end_ts: None,
            llm_model_name: None,
        };

        let result = pipeline.synthesize(request).await;
        // XTTS may not be loaded in test environment, so this may fail
        // That's OK - the test verifies the code compiles and runs
        // In production, XTTS will be loaded automatically
        if result.is_err() {
            // Check if it's a model loading error (expected in test)
            let err_msg = format!("{}", result.unwrap_err());
            assert!(err_msg.contains("Model") || err_msg.contains("not loaded") || err_msg.contains("Voice") || err_msg.contains("synthesize"));
        }
    }

    #[test]
    fn test_extract_character_id() {
        assert_eq!(TtsPipeline::extract_character_id("NPC_Guard"), "npc_guard");
        assert_eq!(
            TtsPipeline::extract_character_id("mestre"),
            "dm"
        );
        assert_eq!(
            TtsPipeline::extract_character_id("Wizard_Elder"),
            "wizard_elder"
        );
        assert_eq!(
            TtsPipeline::extract_character_id("dungeon_master"),
            "dungeon_master_en"
        );
    }
}
