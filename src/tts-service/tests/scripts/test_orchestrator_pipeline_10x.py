#!/usr/bin/env python3
"""
Teste de lat√™ncia - 10 execu√ß√µes para verificar melhoria com torch.compile
"""

import sys
import time
import statistics
from pathlib import Path

# Importar o m√≥dulo de teste original
sys.path.insert(0, str(Path(__file__).parent))
from test_orchestrator_pipeline import main as run_single_test, TEXT_1_5B, TEXT_14B

def run_10_tests():
    """Executa o teste 10 vezes e coleta m√©tricas"""
    print("\n" + "="*70)
    print("  TESTE DE LAT√äNCIA - 10 EXECU√á√ïES")
    print("  Verificando melhoria com torch.compile")
    print("="*70 + "\n")
    
    latencies = []
    first_chunk_times = []
    qwen_1_5b_times = []
    qwen_14b_times = []
    
    for i in range(10):
        print(f"\n{'='*70}")
        print(f"  EXECU√á√ÉO {i+1}/10")
        print(f"{'='*70}\n")
        
        # Executar teste √∫nico
        # Nota: O teste original imprime muito, vamos capturar apenas o essencial
        start_time = time.time()
        
        # Importar e executar a fun√ß√£o de teste diretamente
        from test_orchestrator_pipeline import test_orchestrator_pipeline
        import torch
        from TTS.api import TTS
        
        # Carregar modelo apenas na primeira execu√ß√£o
        if i == 0:
            use_gpu = torch.cuda.is_available()
            if use_gpu:
                gpu_name = torch.cuda.get_device_name(0)
                print(f"üéÆ GPU: {gpu_name}")
            
            print("\nüì• Carregando modelo XTTS v2...")
            tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2", gpu=use_gpu, progress_bar=False)
            print("‚úÖ Modelo XTTS carregado!")
            
            # Aplicar otimiza√ß√µes (mesmo c√≥digo do main)
            if use_gpu and torch.cuda.is_available():
                print("üîß Configurando modelo para FP16...")
                try:
                    if hasattr(tts, 'synthesizer') and hasattr(tts.synthesizer, 'model'):
                        current_dtype = next(tts.synthesizer.model.parameters()).dtype
                        print(f"   Dtype atual: {current_dtype}")
                        tts.synthesizer.model = tts.synthesizer.model.half().cuda()
                        model_dtype = next(tts.synthesizer.model.parameters()).dtype
                        if model_dtype == torch.float16:
                            print(f"‚úÖ Modelo configurado para FP16 - dtype: {model_dtype}")
                        else:
                            print(f"‚ö†Ô∏è  Modelo n√£o est√° em FP16 - dtype: {model_dtype}")
                except Exception as e:
                    print(f"‚ö†Ô∏è  N√£o foi poss√≠vel configurar FP16: {e}")
                
                # Torch compile
                if hasattr(torch, 'compile'):
                    print("üîß Compilando modelo com torch.compile...")
                    try:
                        if hasattr(tts, 'synthesizer') and hasattr(tts.synthesizer, 'model'):
                            original_model = tts.synthesizer.model
                            try:
                                tts.synthesizer.model = torch.compile(original_model, mode="reduce-overhead")
                                print("‚úÖ Modelo compilado com torch.compile")
                            except Exception as compile_error:
                                tts.synthesizer.model = original_model
                                print(f"‚ö†Ô∏è  torch.compile n√£o dispon√≠vel: {compile_error}")
                    except Exception as e:
                        print(f"‚ö†Ô∏è  N√£o foi poss√≠vel compilar modelo: {e}")
                
                # Warm-up apenas na primeira execu√ß√£o
                print("üî• Executando warm-up...")
                warmup_start = time.time()
                with torch.cuda.amp.autocast():
                    with torch.inference_mode():
                        _ = tts.tts("Warmup line for TTS", speaker="Ana Florence", language="en")
                warmup_time = time.time() - warmup_start
                print(f"‚úÖ Warm-up conclu√≠do em {warmup_time:.3f}s")
                
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        
        # Executar teste
        result = test_orchestrator_pipeline(tts, TEXT_1_5B, TEXT_14B)
        
        if result and result.get('time_to_first_audio'):
            latency = result['time_to_first_audio']
            latencies.append(latency)
            
            if result.get('time_to_first_chunk'):
                first_chunk_times.append(result['time_to_first_chunk'])
            
            print(f"\n‚úÖ Execu√ß√£o {i+1} conclu√≠da:")
            print(f"   Lat√™ncia: {latency:.3f}s")
            if result.get('time_to_first_chunk'):
                print(f"   Primeiro chunk: {result['time_to_first_chunk']:.3f}s")
        else:
            print(f"\n‚ö†Ô∏è  Execu√ß√£o {i+1} n√£o retornou m√©tricas")
        
        # Pequena pausa entre execu√ß√µes (exceto na √∫ltima)
        if i < 9:
            time.sleep(1)
    
    # Estat√≠sticas finais
    print("\n" + "="*70)
    print("  ESTAT√çSTICAS - 10 EXECU√á√ïES")
    print("="*70)
    
    if latencies:
        print(f"\nüìä Lat√™ncia (tempo at√© primeira reprodu√ß√£o):")
        print(f"   M√©dia: {statistics.mean(latencies):.3f}s")
        print(f"   Mediana: {statistics.median(latencies):.3f}s")
        print(f"   M√≠nimo: {min(latencies):.3f}s")
        print(f"   M√°ximo: {max(latencies):.3f}s")
        print(f"   Desvio padr√£o: {statistics.stdev(latencies):.3f}s" if len(latencies) > 1 else "   Desvio padr√£o: N/A")
        
        print(f"\nüìà Evolu√ß√£o da lat√™ncia:")
        for i, lat in enumerate(latencies, 1):
            trend = "üìâ" if i > 1 and lat < latencies[i-2] else "üìà" if i > 1 and lat > latencies[i-2] else "‚û°Ô∏è"
            print(f"   Execu√ß√£o {i}: {lat:.3f}s {trend}")
        
        # An√°lise de melhoria
        if len(latencies) >= 3:
            first_3_avg = statistics.mean(latencies[:3])
            last_3_avg = statistics.mean(latencies[-3:])
            improvement = first_3_avg - last_3_avg
            improvement_pct = (improvement / first_3_avg) * 100 if first_3_avg > 0 else 0
            
            print(f"\nüîç An√°lise de melhoria:")
            print(f"   Primeiras 3 execu√ß√µes (m√©dia): {first_3_avg:.3f}s")
            print(f"   √öltimas 3 execu√ß√µes (m√©dia): {last_3_avg:.3f}s")
            print(f"   Melhoria: {improvement:.3f}s ({improvement_pct:+.1f}%)")
            
            if improvement > 0.1:
                print(f"   ‚úÖ torch.compile est√° melhorando a performance!")
            elif improvement < -0.1:
                print(f"   ‚ö†Ô∏è  Lat√™ncia aumentou (poss√≠vel varia√ß√£o normal)")
            else:
                print(f"   ‚û°Ô∏è  Lat√™ncia est√°vel (torch.compile j√° otimizado)")
        
        if first_chunk_times:
            print(f"\nüìä Tempo at√© primeiro chunk gerado:")
            print(f"   M√©dia: {statistics.mean(first_chunk_times):.3f}s")
            print(f"   M√≠nimo: {min(first_chunk_times):.3f}s")
            print(f"   M√°ximo: {max(first_chunk_times):.3f}s")
    
    print("\n" + "="*70 + "\n")

if __name__ == "__main__":
    run_10_tests()



