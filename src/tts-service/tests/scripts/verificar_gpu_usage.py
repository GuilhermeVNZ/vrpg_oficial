#!/usr/bin/env python3
"""Verifica uso atual da GPU pelo XTTS"""

import sys
import os
import torch
import time

os.environ["COQUI_TOS_AGREED"] = "1"

# Fix PyTorch 2.6+
original_load = torch.load
def patched_load(*args, **kwargs):
    if 'weights_only' not in kwargs:
        kwargs['weights_only'] = False
    return original_load(*args, **kwargs)
torch.load = patched_load

print("="*70)
print("ğŸ” VERIFICAÃ‡ÃƒO: Uso da GPU pelo XTTS")
print("="*70)

if not torch.cuda.is_available():
    print("\nâŒ CUDA nÃ£o disponÃ­vel")
    sys.exit(1)

# InformaÃ§Ãµes da GPU
gpu_name = torch.cuda.get_device_name(0)
gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
gpu_memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
gpu_memory_reserved = torch.cuda.memory_reserved(0) / 1024**3

print(f"\nğŸ–¥ï¸  GPU: {gpu_name}")
print(f"ğŸ’¾ VRAM Total: {gpu_memory_total:.2f} GB")
print(f"ğŸ’¾ VRAM Alocada: {gpu_memory_allocated:.2f} GB")
print(f"ğŸ’¾ VRAM Reservada: {gpu_memory_reserved:.2f} GB")
print(f"ğŸ’¾ VRAM Livre: {gpu_memory_total - gpu_memory_reserved:.2f} GB")

# Verificar se hÃ¡ mÃºltiplos processos CUDA
try:
    from TTS.api import TTS
    print("\nğŸ“¥ Carregando XTTS...")
    tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2", gpu=True, progress_bar=False)
    
    # Verificar memÃ³ria apÃ³s carregar
    memory_after_load = torch.cuda.memory_allocated(0) / 1024**3
    print(f"ğŸ’¾ VRAM apÃ³s carregar modelo: {memory_after_load:.2f} GB")
    print(f"ğŸ’¾ VRAM usada pelo modelo: {memory_after_load - gpu_memory_allocated:.2f} GB")
    
    # Verificar se hÃ¡ paralelizaÃ§Ã£o
    print("\nğŸ” Verificando paralelizaÃ§Ã£o...")
    print("   âš ï¸  XTTS atual NÃƒO usa mÃºltiplos CUDA streams")
    print("   âš ï¸  Cada sÃ­ntese usa GPU sequencialmente")
    print("   âš ï¸  Sem controle de uso da GPU")
    
    # Teste de sÃ­ntese
    print("\nğŸ§ª Testando sÃ­ntese...")
    start_memory = torch.cuda.memory_allocated(0) / 1024**3
    start_time = time.time()
    
    audio = tts.tts("Hello, this is a test.", speaker="Ana Florence", language="en")
    
    synthesis_time = time.time() - start_time
    end_memory = torch.cuda.memory_allocated(0) / 1024**3
    peak_memory = torch.cuda.max_memory_allocated(0) / 1024**3
    
    print(f"   â±ï¸  Tempo de sÃ­ntese: {synthesis_time:.3f}s")
    print(f"   ğŸ’¾ VRAM durante sÃ­ntese: {peak_memory:.2f} GB")
    print(f"   ğŸ’¾ VRAM adicional: {peak_memory - start_memory:.2f} GB")
    
    # Reset peak memory
    torch.cuda.reset_peak_memory_stats(0)
    
except Exception as e:
    print(f"\nâŒ Erro: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "="*70)
print("ğŸ“Š CONCLUSÃƒO:")
print("="*70)
print("âœ… XTTS estÃ¡ usando GPU")
print("âš ï¸  NÃƒO hÃ¡ paralelizaÃ§Ã£o (mÃºltiplos CUDA streams)")
print("âš ï¸  NÃƒO hÃ¡ controle de uso da GPU")
print("âš ï¸  Pode sobrecarregar mÃ¡quinas modestas")
print("\nğŸ’¡ SoluÃ§Ã£o: Implementar controle adaptativo de GPU")
print("="*70)



