#!/usr/bin/env python3
"""
Teste para verificar se FP16 est√° totalmente ativo no modelo XTTS
"""

import sys
import torch

# --- Fix para PyTorch 2.6+ ---
original_load = torch.load
def patched_load(*args, **kwargs):
    if 'weights_only' not in kwargs:
        kwargs['weights_only'] = False
    return original_load(*args, **kwargs)
torch.load = patched_load

# Adicionar safe globals
try:
    from TTS.tts.configs.xtts_config import XttsConfig
    torch.serialization.add_safe_globals([XttsConfig])
except:
    pass

from TTS.api import TTS

def test_fp16_verification():
    """Testa se FP16 est√° realmente ativo no modelo"""
    print("\n" + "="*70)
    print("  TESTE: VERIFICA√á√ÉO FP16")
    print("="*70)
    
    use_gpu = torch.cuda.is_available()
    if not use_gpu:
        print("\n‚ùå GPU n√£o dispon√≠vel - FP16 requer GPU")
        return False
    
    gpu_name = torch.cuda.get_device_name(0)
    print(f"\nüéÆ GPU: {gpu_name}")
    
    # Carregar modelo
    print("\nüì• Carregando modelo XTTS v2...")
    try:
        tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2", gpu=use_gpu, progress_bar=False)
        print("‚úÖ Modelo XTTS carregado!")
    except Exception as e:
        print(f"‚ùå ERRO ao carregar modelo: {e}")
        return False
    
    # Verificar dtype antes da convers√£o
    print("\nüîç Verificando dtype ANTES da convers√£o...")
    try:
        # Tentar diferentes caminhos para acessar o modelo
        model = None
        if hasattr(tts, 'synthesizer') and hasattr(tts.synthesizer, 'model'):
            model = tts.synthesizer.model
        elif hasattr(tts, 'model'):
            model = tts.model
        elif hasattr(tts, 'synthesizer'):
            # Tentar acessar atrav√©s de outros atributos
            synth = tts.synthesizer
            if hasattr(synth, 'model'):
                model = synth.model
            elif hasattr(synth, 'tts_model'):
                model = synth.tts_model
        
        if model is None:
            print("‚ö†Ô∏è  N√£o foi poss√≠vel acessar o modelo diretamente")
            print("   Tentando m√©todo alternativo...")
            # Tentar atrav√©s de warm-up para verificar dtype durante infer√™ncia
            print("   Executando warm-up para verificar dtype...")
            with torch.inference_mode():
                _ = tts.tts("Test", speaker="Ana Florence", language="en")
            # Tentar novamente ap√≥s warm-up
            if hasattr(tts, 'synthesizer') and hasattr(tts.synthesizer, 'model'):
                model = tts.synthesizer.model
        
        if model is not None:
            first_param = next(model.parameters())
            dtype_before = first_param.dtype
            device_before = first_param.device
            
            print(f"   Dtype: {dtype_before}")
            print(f"   Device: {device_before}")
            
            # Contar par√¢metros por dtype
            param_count = {}
            for p in model.parameters():
                dt = str(p.dtype)
                param_count[dt] = param_count.get(dt, 0) + 1
            
            print(f"   Par√¢metros por dtype: {param_count}")
        else:
            print("‚ö†Ô∏è  N√£o foi poss√≠vel acessar o modelo")
            print("   Vamos tentar converter mesmo assim...")
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao verificar modelo: {e}")
        print("   Continuando com convers√£o...")
        model = None
    
    # Converter para FP16
    print("\nüîß Convertendo modelo para FP16...")
    try:
        # Tentar diferentes caminhos
        converted = False
        if hasattr(tts, 'synthesizer') and hasattr(tts.synthesizer, 'model'):
            tts.synthesizer.model = tts.synthesizer.model.half().cuda()
            converted = True
            print("‚úÖ Convers√£o aplicada (.half().cuda()) via synthesizer.model")
        elif hasattr(tts, 'model'):
            tts.model = tts.model.half().cuda()
            converted = True
            print("‚úÖ Convers√£o aplicada via tts.model")
        else:
            print("‚ö†Ô∏è  N√£o foi poss√≠vel acessar o modelo para convers√£o")
            print("   O modelo pode ser convertido internamente durante infer√™ncia")
            converted = False
        
        if not converted:
            print("‚ö†Ô∏è  Continuando sem convers√£o expl√≠cita (pode usar autocast)")
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao converter modelo: {e}")
        print("   Continuando com autocast como fallback")
        converted = False
    
    # Verificar dtype ap√≥s convers√£o
    print("\nüîç Verificando dtype AP√ìS a convers√£o...")
    try:
        # Tentar acessar modelo novamente
        model = None
        if hasattr(tts, 'synthesizer') and hasattr(tts.synthesizer, 'model'):
            model = tts.synthesizer.model
        elif hasattr(tts, 'model'):
            model = tts.model
        
        if model is not None:
            first_param = next(model.parameters())
            dtype_after = first_param.dtype
            device_after = first_param.device
            
            print(f"   Dtype: {dtype_after}")
            print(f"   Device: {device_after}")
            
            # Verificar todos os par√¢metros
            all_fp16 = True
            param_count_after = {}
            fp16_count = 0
            total_count = 0
            
            for p in model.parameters():
                dt = str(p.dtype)
                param_count_after[dt] = param_count_after.get(dt, 0) + 1
                total_count += 1
                if p.dtype == torch.float16:
                    fp16_count += 1
                else:
                    all_fp16 = False
            
            print(f"   Par√¢metros por dtype: {param_count_after}")
            print(f"   Par√¢metros FP16: {fp16_count}/{total_count} ({fp16_count/total_count*100:.1f}%)")
            
            if dtype_after == torch.float16 and all_fp16:
                print("\n‚úÖ FP16 TOTALMENTE ATIVO!")
                print("   Todos os par√¢metros est√£o em torch.float16")
                return True
            elif dtype_after == torch.float16:
                print("\n‚ö†Ô∏è  FP16 PARCIALMENTE ATIVO")
                print(f"   Primeiro par√¢metro em FP16, mas {total_count - fp16_count} par√¢metros n√£o est√£o")
                return False
            else:
                print("\n‚ùå FP16 N√ÉO ATIVO")
                print(f"   Dtype atual: {dtype_after}")
                return False
        else:
            print("‚ö†Ô∏è  N√£o foi poss√≠vel acessar o modelo ap√≥s convers√£o")
            print("   Vamos testar com infer√™ncia para verificar comportamento")
            return None  # Indeterminado, precisa testar com infer√™ncia
    except Exception as e:
        print(f"‚ö†Ô∏è  Erro ao verificar modelo: {e}")
        return None
    
    # Teste de infer√™ncia
    print("\nüß™ Testando infer√™ncia com FP16...")
    try:
        with torch.inference_mode():
            audio = tts.tts("Test line for FP16 verification", speaker="Ana Florence", language="en")
        print("‚úÖ Infer√™ncia bem-sucedida")
        
        # Verificar dtype novamente ap√≥s infer√™ncia
        model_dtype = next(tts.synthesizer.model.parameters()).dtype
        print(f"   Dtype ap√≥s infer√™ncia: {model_dtype}")
        
        if model_dtype == torch.float16:
            print("‚úÖ FP16 mantido ap√≥s infer√™ncia")
            return True
        else:
            print("‚ö†Ô∏è  FP16 n√£o mantido ap√≥s infer√™ncia")
            return False
    except Exception as e:
        print(f"‚ùå Erro na infer√™ncia: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_fp16_verification()
    print("\n" + "="*70)
    if success:
        print("‚úÖ TESTE PASSOU - FP16 est√° totalmente ativo")
    else:
        print("‚ùå TESTE FALHOU - FP16 n√£o est√° totalmente ativo")
    print("="*70 + "\n")
    sys.exit(0 if success else 1)

