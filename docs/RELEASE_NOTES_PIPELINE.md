# Release Notes - Pipeline de 3 Agentes

## VersÃ£o 2.0.0 - Pipeline de 3 Agentes

### ğŸ‰ Nova Arquitetura

Esta versÃ£o introduz uma **arquitetura completamente nova** com Pipeline de 3 Agentes para melhorar drasticamente a latÃªncia e qualidade das respostas.

### âœ¨ Principais MudanÃ§as

#### Pipeline de 3 Agentes
- **Orquestrador**: LÃ³gica determinÃ­stica que coordena todos os componentes
- **Qwen-1.5B**: Modelo rÃ¡pido para reaÃ§Ãµes imediatas (< 1.2s)
- **Qwen-14B**: Modelo completo para narrativa detalhada (< 6s)

#### OtimizaÃ§Ãµes de LatÃªncia
- **Respostas Objetivas**: InstantÃ¢neas (< 50ms) - sem uso de LLM
- **Regras Simples**: RÃ¡pidas (< 1.5s) - apenas Vectorizer + 1.5B
- **Narrativas Completas**: < 6s - com reaÃ§Ã£o inicial do 1.5B

#### Novos Recursos

1. **Intent Router Inteligente**
   - Classifica automaticamente o tipo de pergunta
   - Roteia para o melhor caminho de processamento
   - Cache para perguntas frequentes

2. **Sistema de Caches**
   - **Game State Cache**: Estado do jogo em RAM
   - **Scene Context Cache**: Contexto da cena recente
   - **Lore Cache**: InformaÃ§Ãµes de lore do Vectorizer

3. **Sistema de PersistÃªncia de SessÃ£o**
   - Save/Load completo de sessÃµes
   - PreservaÃ§Ã£o de estado entre sessÃµes
   - Versionamento de formato

4. **Respostas Objetivas**
   - Perguntas sobre HP, AC, recursos respondidas diretamente
   - Sem necessidade de LLM para dados factuais
   - LatÃªncia ultra-baixa

### ğŸ”§ MudanÃ§as TÃ©cnicas

#### Requisitos de Hardware
- **MÃ­nimo**: 16GB RAM, GPU com 8GB VRAM
- **Recomendado**: 32GB RAM, GPU com 16GB+ VRAM
- **Ideal**: 64GB RAM, GPU com 24GB+ VRAM

#### Modelos NecessÃ¡rios
- **Qwen-1.5B**: `qwen2.5-1.5b-instruct-q4_k_m.gguf` (~1GB)
- **Qwen-14B**: `qwen2.5-14b-instruct-q4_k_m.gguf` (~8GB)

Ambos modelos devem estar na pasta:
```
assets-and-models/models/llm/
```

#### ConfiguraÃ§Ã£o

Novo formato de `config/llm_config.json`:
```json
{
  "models": {
    "1_5b": {
      "path": "assets-and-models/models/llm/qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "max_tokens": 40,
      "temperature": 0.8,
      "top_p": 0.9
    },
    "14b": {
      "path": "assets-and-models/models/llm/qwen2.5-14b-instruct-q4_k_m.gguf",
      "max_tokens": 512,
      "temperature": 0.7,
      "top_p": 0.95
    }
  },
  "memory": {
    "keep_both_loaded": true,
    "preload_on_startup": true
  }
}
```

### ğŸ“š DocumentaÃ§Ã£o

DocumentaÃ§Ã£o completa disponÃ­vel:
- **[USER_GUIDE_PIPELINE.md](USER_GUIDE_PIPELINE.md)**: Guia completo para usuÃ¡rios
- **[MODEL_CONFIGURATION_GUIDE.md](MODEL_CONFIGURATION_GUIDE.md)**: Guia de configuraÃ§Ã£o dos modelos
- **[TROUBLESHOOTING_PIPELINE.md](TROUBLESHOOTING_PIPELINE.md)**: Guia de troubleshooting
- **[MIGRATION_GUIDE.md](MIGRATION_GUIDE.md)**: Guia de migraÃ§Ã£o da versÃ£o anterior

### ğŸš€ MigraÃ§Ã£o

Se vocÃª estÃ¡ usando uma versÃ£o anterior:

1. **Backup de dados**: FaÃ§a backup da pasta `saves/` se existir
2. **Baixe modelos**: VocÃª precisa de ambos modelos (1.5B e 14B)
3. **Atualize configuraÃ§Ã£o**: Use o novo formato de `llm_config.json`
4. **Migre sessÃµes**: SessÃµes antigas serÃ£o migradas automaticamente

Consulte **[MIGRATION_GUIDE.md](MIGRATION_GUIDE.md)** para detalhes completos.

### ğŸ› CorreÃ§Ãµes

- CorreÃ§Ã£o de latÃªncia alta em respostas objetivas
- Melhoria na classificaÃ§Ã£o de intenÃ§Ãµes
- OtimizaÃ§Ã£o de uso de memÃ³ria com ambos modelos

### ğŸ“Š Testes

- **11 testes de persistÃªncia**: 100% passando
- **11 testes de integraÃ§Ã£o**: 100% passando
- **8 benchmarks de performance**: Todos dentro dos targets
- **15 testes de regressÃ£o**: Nenhuma regressÃ£o identificada

### ğŸ”„ Compatibilidade

- **SessÃµes antigas**: MigraÃ§Ã£o automÃ¡tica suportada
- **ConfiguraÃ§Ãµes**: Formato antigo ainda funciona, mas novo formato recomendado
- **APIs**: Principais APIs mantÃªm compatibilidade

### ğŸ“ Notas de VersÃ£o

#### Breaking Changes
- Novo formato de `llm_config.json` (formato antigo ainda funciona com aviso)
- Ambos modelos necessÃ¡rios (1.5B e 14B)

#### Deprecations
- Nenhuma depreciaÃ§Ã£o nesta versÃ£o

#### Security
- Nenhuma mudanÃ§a de seguranÃ§a nesta versÃ£o

### ğŸ™ Agradecimentos

Agradecemos a todos os usuÃ¡rios que testaram e forneceram feedback durante o desenvolvimento desta versÃ£o.

### ğŸ“ Suporte

Se encontrar problemas:
1. Consulte **[TROUBLESHOOTING_PIPELINE.md](TROUBLESHOOTING_PIPELINE.md)**
2. Abra uma issue no GitHub
3. Entre em contato via email: suporte@vrpg-client.com

---

**VRPG Client 2.0.0** - Pipeline de 3 Agentes  
*Transformando a experiÃªncia de RPG com IA local e tecnologia de ponta!* ğŸ²âœ¨













