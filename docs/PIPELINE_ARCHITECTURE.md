# VRPG - Arquitetura de Pipeline com 3 Agentes

## üìò Vis√£o Geral

O objetivo do orquestrador √© garantir **tempo de resposta real-time** sem sacrificar **qualidade narrativa**.

Isso √© feito dividindo a pipeline em **3 agentes cognitivos + camadas determin√≠sticas**:

```
Orquestrador ‚Üí l√≥gica pura, determin√≠stica
Qwen-1.5B ‚Üí rea√ß√£o humana inicial ("prel√∫dio")
Qwen-14B ‚Üí narrativa, consequ√™ncia, resolu√ß√£o
```

### Regra Central

**Nenhuma resposta final √© emitida pelo 14B at√© que o 1.5B j√° tenha iniciado a resposta.**

---

## üß† Pap√©is (Sem Ambiguidade)

### 1. ORQUESTRADOR (Autoridade Absoluta)

**Conecta os componentes**

**Controla quem responde e quando**

**N√£o "raciocina"**

**N√£o inventa regras**

**Responde perguntas objetivas sozinho**

**Garante que o 1.5B sempre responde antes do 14B**

#### NUNCA chamar LLM quando:

- Pergunta factual
- Estado de jogo
- Logs diretos
- Regras simples e un√≠vocas

**Exemplo**:

```
"Quantos slots n√≠vel 2 eu tenho?"

Orquestrador:
  return player.spell_slots.level2

Zero Qwen. Zero vectorizer. Zero drama.
```

---

### 2. QWEN 1.5B ‚Äî "MESTRE REFLEXO"

Serve para **preencher sil√™ncio** e **simular rea√ß√£o humana imediata**.

#### Fun√ß√µes Permitidas:

- ‚úÖ Rea√ß√£o emocional curta
- ‚úÖ Ack da inten√ß√£o
- ‚úÖ Mini-narra√ß√£o inconclusiva
- ‚úÖ Perguntas simples de follow-up
- ‚úÖ Clarifica√ß√£o ("Voc√™ disse goblin da esquerda?")

#### Fun√ß√µes PROIBIDAS:

- ‚ùå Resultado final
- ‚ùå An√°lise de sistemas
- ‚ùå Consequ√™ncias
- ‚ùå Aplica√ß√£o de regras
- ‚ùå Qualquer julgamento do tipo "acertou/errou"
- ‚ùå Narrativa de 2¬∫ ato

#### Estilo de Resposta:

- **1 ou 2 frases**
- **Nunca repetitivas**
- **Nunca formulaicas**
- **N√£o "enche lingui√ßa"**
- **Deve abrir espa√ßo narrativo**

#### Exemplos:

**BOM**:
```
"Interessante. Voc√™ segura firme a l√¢mina, sentindo o calor da batalha."
```

**RUIM**:
```
"Interessante‚Ä¶ voc√™ corre‚Ä¶ ok‚Ä¶ certo‚Ä¶ certo‚Ä¶ t√°‚Ä¶"
```

O 1.5B cria **GRAVIDADE** ‚Äî n√£o "PREENCHIMENTO".

---

### 3. QWEN 14B ‚Äî "MESTRE REAL"

√â o **autor**. O **diretor**. O que **resolve a cena**.

#### Fun√ß√µes Permitidas:

- ‚úÖ Descrever a cena com riqueza
- ‚úÖ Consequ√™ncias de a√ß√µes
- ‚úÖ Falhas cr√≠ticas / sucessos cr√≠ticos
- ‚úÖ Rea√ß√µes de NPCs
- ‚úÖ Pedidos de teste ("role ataque")
- ‚úÖ Aplica√ß√£o de regras dentro do contexto
- ‚úÖ Integra√ß√£o de lore / mem√≥ria
- ‚úÖ Avan√ßo da hist√≥ria

#### PROIBIDO:

- ‚ùå Repetir texto do 1.5B
- ‚ùå Contradizer 1.5B
- ‚ùå Resetar o contexto da a√ß√£o
- ‚ùå Explicar regra como manual (a n√£o ser que seja a pedido)

---

## üß¨ Estado e Cache (N√∫cleo do Sistema)

### O que armazenamos no cache:

#### 1. Contexto de Combate:

- Round atual
- Iniciativa
- HP por entidade
- Status: "poisoned", "stealth", "prone", etc
- Localiza√ß√£o (grid 2D/3D)
- Buffs / debuffs
- Recursos (rage, slots, smites, ki)

#### 2. Hist√≥rico Curto (√∫ltimas 3‚Äì8 a√ß√µes)

Por turno do jogador:

- A√ß√£o
- Resultado
- Teste executado
- Intera√ß√£o com NPC

**üõë N√£o armazene hist√≥rico gigantesco no contexto do prompt.**

Use vector search de mem√≥ria epis√≥dica.

#### 3. Mem√≥ria de Lore (vectorizer)

- Descri√ß√£o de ra√ßas
- Cidade / regi√µes / dungeons
- NPCs recorrentes
- Hist√≥ria da campanha
- √Åreas, fac√ß√µes, cren√ßas
- Estilo narrativo desejado

**LLM consulta vectorizer, n√£o inventa.**

#### 4. Mem√≥ria do Jogador

- Classe
- Invent√°rio
- Magias preparadas
- Habilidades
- Per√≠cias
- Defeitos / motiva√ß√£o / background

#### Nada de calcular via LLM:

Dano, HP, Armor Class, iniciativa, dura√ß√£o de efeitos ‚Üí **estado matem√°tico puro**.

---

## üß≠ Fluxo T√©cnico: Urgente + Cristalino

### SITUA√á√ÉO: JOGADOR DECLARA A√á√ÉO

**Exemplo**:
```
"Corro pela lateral da mesa e corto a garganta do goblin."
```

### ETAPA 1 ‚Äî STT STREAMING

- Whisper processa em chunks (300‚Äì600ms)
- Orquestrador recebe `asr_partial`

### ETAPA 2 ‚Äî INTENT ROUTER

**Simples**:

```
INTENT = ACTION
ENTITY = goblin
VERB = attack
WEAPON = sword
MOVEMENT = lateral
```

### ETAPA 3 ‚Äî DISPARO AUTOM√ÅTICO DO 1.5B

**Condi√ß√£o**:

- Fala passou 6‚Äì8 segundos
- **OU**
- Pausa detectada
- **OU**
- A√ß√£o clara identificada

**Prompt do 1.5B**:

- M√°x. 25‚Äì40 tokens
- Estilo emocional
- Zero consequ√™ncia
- Zero regra
- Zero resultado

**BOM**:
```
"Interessante. Voc√™ segura firme a l√¢mina, sentindo o calor da batalha."
```

Vai direto para **XTTS Streaming Pipeline** (n√£o mais SoVITS).

### ETAPA 3.5 ‚Äî XTTS STREAMING REAL-TIME

**Pipeline de Streaming**:

1. **Semantic Chunker**: Divide texto por pausas narrativas (3-7s, 180-320 chars)
2. **XTTS Worker** (Thread C): Gera chunks em paralelo (High-End) ou sequencial (Modest)
3. **AudioBuffer FIFO**: Thread-safe, Float32 interno, int16 I/O
4. **Pre-Buffer Manager**: Mant√©m 1-2 chunks √† frente (tier-dependent)
5. **Audio Output** (Thread D): Thread dedicada, WASAPI/ASIO/CoreAudio, 256-512 frames

**Controle Adaptativo de GPU**:
- Detecta hardware automaticamente (High-End/Mid-Range/Modest/Low-End)
- Aplica configura√ß√£o apropriada (paraleliza√ß√£o, VRAM limit, pre-buffer)
- Mant√©m performance em todos os tiers (< 5s lat√™ncia inicial)

**Otimiza√ß√µes de √Åudio**:
- Sample rate: 16-24 kHz (n√£o 48 kHz)
- Channels: Mono (1 canal, n√£o est√©reo)
- Buffer: 256-512 frames (n√£o 2048/4096)
- Formato: Float32 interno, int16 I/O

### ETAPA 4 ‚Äî ESPERAR O JOGADOR TERMINAR

Quando Whisper fecha `asr_final`, orquestrador prepara prompt do 14B:

**NELE**:

- `fast_prelude` (texto 1.5B)
- `asr_final`
- `game_state`
- `context_slice` (√∫ltimos 3‚Äì6 eventos)
- `vectorizer results` (se relevante)
- liga√ß√£o com a cena

**E o 14B produz**:

```
"...com um impulso s√∫bito voc√™ avan√ßa pela lateral.
O goblin tenta erguer o punhal, mas tarde demais‚Äî
Fa√ßa uma rolagem de ataque."
```

### üö´ N√ÉO PODE JAMAIS ACONTECER

**Qwen 1.5B**:
```
"Voc√™ corta a garganta dele."
```

**Isso √© o 14B.**

---

## üìê Consulta de Regras

### A) Pergunta Objetiva (Orquestrador)

```
"Quantos slots de magia de n√≠vel 3 eu tenho?"
```

**Responde direto dos dados**:

```rust
return player.slots.level3
```

### B) Pergunta de Regra Simples (vectorizer + 1.5B)

```
"Stealth usa Destreza?"
```

**Vectorizer busca defini√ß√£o exata**:

```
Stealth ‚Äï habilidade baseada em Destreza.
```

**1.5B converte em resposta humana**:

```
"Stealth usa Destreza. Investigation √© Intelig√™ncia."
```

### C) Pergunta que Impacta Narrativa

```
"Se eu pular do balc√£o e tentar acertar pelas costas, ganho vantagem?"
```

**14B entra porque**:

- posi√ß√£o
- movimento
- surpresa
- rea√ß√£o do inimigo
- tens√£o

---

## üí£ O Ponto que Voc√™ N√£o Quer Errar

**O 1.5B N√ÉO ENCHE LINGUI√áA.**

Ele cria **"GRAVIDADE"** ‚Äî n√£o **"PREENCHIMENTO"**.

**Exemplo errado (IA t√≠pica)**:
```
"Interessante‚Ä¶ voc√™ corre‚Ä¶ ok‚Ä¶ certo‚Ä¶ certo‚Ä¶ t√°‚Ä¶"
```

**Horr√≠vel. Mec√¢nico. Artificial.**

**Exemplo correto (humano experiente)**:
```
"Voc√™ inspira fundo. Essa decis√£o diz muito sobre voc√™."
```

**Frase pequena, densa, humana.**

---

## ‚öôÔ∏è Design Anti-Loop

### 1. Banco Local com 50‚Äì300 Frases de "Ponte Humana"

Divididas por emo√ß√£o

Aleatorizadas

Nunca repetitivas

**Exemplos**:

- "Hmmm‚Ä¶ ousado."
- "Voc√™ escolhe a via dif√≠cil."
- "Isso vai ser interessante."
- "Vamos ver at√© onde isso vai."

**O 1.5B escolhe, n√£o inventa.**

---

## üåå Onde a Arquitetura Quebra (Casos Reais)

### üî• Erro 1 ‚Äî 1.5B narrar demais

Ele vira mini-mestre.

1.5B vira lixo.

14B vira p√≥s-produtor.

### üî• Erro 2 ‚Äî 14B entrar frio

Sem `fast_prelude`, ele gasta tokens:

- recap
- framing
- setup
- emo√ß√£o

**Lat√™ncia 2‚Äì5s ‚Üí horr√≠vel.**

### üî• Erro 3 ‚Äî Falta de Cache

14B precisa reprocessar contexto ‚Üí 3‚Äì9s

### üî• Erro 4 ‚Äî 1.5B virar manual de regras

Jogador sente "chatbot wiki".

### üî• Erro 5 ‚Äî Orquestrador Fraco

LLM decide a pr√≥pria fun√ß√£o.

**Resultado: caos.**

---

## ‚ö° Lat√™ncia Real

### 1.5B (Prelude)

- parse intent: 30‚Äì80ms
- gera√ß√£o: 200‚Äì450ms
- XTTS streaming: 150‚Äì300ms (primeiro chunk)
- **Total**: 380‚Äì830ms

**üëâ Resposta inicial < 1.2s**

### 14B (Narrative - Streaming)

- ingest contexto: 200‚Äì500ms
- gera√ß√£o narrativa: 1.5‚Äì4s (texto completo)
- Semantic chunking: 10‚Äì50ms
- XTTS streaming: 1.2‚Äì2.8s por chunk (RTF 0.4x)
- Pre-buffer: 1.0‚Äì2.5s (tier-dependent)

**üëâ Lat√™ncia inicial: 2.5‚Äì4.0s (todos os tiers)**
**üëâ Streaming cont√≠nuo: Zero gaps, playback fluido**

### Performance por GPU Tier

| Tier | Lat√™ncia Inicial | RTF | GPU Usage | Pre-Buffer |
|------|------------------|-----|-----------|------------|
| **High-End** | 2.5-3.8s | < 0.5x | 80-95% | 2.5s |
| **Mid-Range** | 2.5-4.0s | < 0.6x | 60-80% | 1.75s |
| **Modest** | 3.0-4.5s | < 0.8x | 40-60% | 1.25s |
| **Low-End** | 3.5-5.0s | < 1.0x | 30-50% | 0.75s |

---

## üíæ Como Salvar o Cache (Baixo Custo)

### game_state (RAM)

- HP
- AC
- recursos
- status
- cooldowns
- posi√ß√£o
- iniciativa

### scene_context (RAM + Vector)

- √∫ltimas a√ß√µes (3‚Äì6)
- resultado de rolagens
- NPCs ativos
- quem interagiu com quem

### lore_context (Vectorizer)

- queries curtas
- textos originais
- passagens relevantes

---

## üßä √öltima Regra

**Se a pergunta puder ser respondida sem imagina√ß√£o,**

**LLM N√ÉO DEVE SER CHAMADO.**

---

## Implementa√ß√£o T√©cnica

### Estrutura de Dados

```rust
pub struct PipelineState {
    // Estado do jogo (RAM)
    pub game_state: GameState,
    
    // Contexto da cena (RAM + Vector)
    pub scene_context: SceneContext,
    
    // Cache de mem√≥ria (Vectorizer)
    pub lore_cache: LoreCache,
    
    // Estado do pipeline
    pub pipeline_status: PipelineStatus,
}

pub enum PipelineStatus {
    WaitingForInput,
    Processing1_5B,      // 1.5B est√° gerando prel√∫dio
    WaitingForFinalASR,  // Aguardando asr_final
    Processing14B,       // 14B est√° gerando narrativa completa
    ReadyForTTS,
}
```

### Fluxo de Execu√ß√£o (Streaming Real-Time)

```rust
impl Orchestrator {
    async fn handle_player_input(&mut self, asr_partial: &str) -> Result<()> {
        // 1. Parse intent
        let intent = self.parse_intent(asr_partial)?;
        
        // 2. Verificar se deve disparar 1.5B
        if self.should_trigger_1_5b() {
            // Disparar 1.5B em paralelo (Thread A)
            let prelude = self.trigger_1_5b(intent.clone()).await?;
            
            // Enviar para XTTS Streaming Pipeline imediatamente
            // Thread C: XTTS Worker gera chunks
            // Thread D: Audio Consumer toca em streaming
            self.send_to_tts_streaming(prelude).await?;
        }
        
        // 3. Aguardar asr_final
        let asr_final = self.wait_for_final_asr().await?;
        
        // 4. Preparar contexto para 14B
        let context = self.prepare_14b_context(asr_final, intent).await?;
        
        // 5. Gerar narrativa completa com 14B (Thread B)
        let narration = self.trigger_14b(context).await?;
        
        // 6. Enviar para XTTS Streaming Pipeline
        // Semantic Chunker ‚Üí XTTS Worker (Thread C) ‚Üí AudioBuffer FIFO ‚Üí Audio Output (Thread D)
        self.send_to_tts_streaming(narration).await?;
        
        Ok(())
    }
    
    async fn send_to_tts_streaming(&mut self, text: String) -> Result<()> {
        // Thread C: XTTS Worker (adaptive parallel/sequential)
        // - Semantic chunking (3-7s chunks)
        // - GPU adaptive control (tier-based)
        // - Pre-buffering (1-2 chunks ahead)
        // - Push to AudioBuffer FIFO
        
        // Thread D: Audio Consumer (dedicated I/O)
        // - Pop from AudioBuffer FIFO
        // - Convert Float32 to int16
        // - Native audio output (WASAPI/ASIO/CoreAudio)
        // - Zero-gap playback
        
        Ok(())
    }
}
```

### Configura√ß√£o de Modelos

```json
{
  "models": {
    "qwen_1_5b": {
      "path": "assets-and-models/models/llm/qwen2.5-1.5b-instruct-q4_k_m.gguf",
      "max_tokens": 40,
      "temperature": 0.8,
      "top_p": 0.9,
      "role": "prelude"
    },
    "qwen_14b": {
      "path": "assets-and-models/models/llm/qwen2.5-14b-instruct-q4_k_m.gguf",
      "max_tokens": 2048,
      "temperature": 0.7,
      "top_p": 0.9,
      "role": "narration"
    }
  }
}
```

---

## Testes

### Teste 1: 1.5B n√£o deve narrar resultado

**Input**: "Eu ataco o goblin"

**1.5B esperado**: "Voc√™ avan√ßa com determina√ß√£o."

**1.5B n√£o deve**: "Voc√™ acerta o goblin e causa 8 de dano."

### Teste 2: 14B recebe prel√∫dio

**Verificar**: Contexto do 14B cont√©m `fast_prelude` do 1.5B

### Teste 3: Orquestrador responde perguntas objetivas

**Input**: "Quantos HP eu tenho?"

**Esperado**: Resposta direta do estado, sem chamar LLM

### Teste 4: Lat√™ncia do pipeline

**Target**: 
- 1.5B resposta < 1.2s
- 14B resposta < 6s total

---

## Refer√™ncias

- [ORCHESTRATOR.md](ORCHESTRATOR.md) - Especifica√ß√£o t√©cnica do orquestrador
- [LLM_CORE_SPEC.md](specs/LLM_CORE_SPEC.md) - Especifica√ß√£o dos modelos LLM
- [PERFORMANCE.md](PERFORMANCE.md) - M√©tricas de performance e lat√™ncia

