#!/usr/bin/env python3
"""Verifica se Whisper e Qwen est√£o usando GPU"""

import sys
import subprocess
import os

print("=" * 70)
print("üîç Verifica√ß√£o GPU: Whisper e Qwen")
print("=" * 70)
print()

# 1. Verificar Whisper
print("1Ô∏è‚É£ Whisper (ASR):")
print("   Verificando implementa√ß√£o...")

# Verificar se √© whisper.cpp ou faster-whisper
try:
    # Tentar faster-whisper (suporta GPU)
    from faster_whisper import WhisperModel
    print("   ‚úÖ faster-whisper instalado")
    
    # Verificar se pode usar GPU
    import torch
    if torch.cuda.is_available():
        print(f"   üéÆ GPU dispon√≠vel: {torch.cuda.get_device_name(0)}")
        print("   üí° faster-whisper pode usar GPU com device='cuda'")
        print("   ‚ö†Ô∏è  Verifique se est√° configurado para usar GPU no c√≥digo")
    else:
        print("   ‚ö†Ô∏è  GPU n√£o dispon√≠vel no PyTorch")
except ImportError:
    print("   ‚ö†Ô∏è  faster-whisper n√£o instalado")
    print("   üí° Para usar GPU: pip install faster-whisper")
    
    # Verificar whisper.cpp
    try:
        result = subprocess.run(['whisper-cpp', '--help'], 
                              capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            print("   ‚úÖ whisper.cpp encontrado")
            print("   üí° whisper.cpp pode usar GPU com CUDA (se compilado com CUDA)")
        else:
            print("   ‚ö†Ô∏è  whisper.cpp n√£o encontrado")
    except:
        print("   ‚ö†Ô∏è  whisper.cpp n√£o encontrado")

print()

# 2. Verificar Qwen (llama.cpp)
print("2Ô∏è‚É£ Qwen (LLM):")
print("   Verificando implementa√ß√£o...")

# Qwen provavelmente usa llama.cpp (GGUF)
try:
    # Verificar se llama.cpp est√° dispon√≠vel
    result = subprocess.run(['llama-cli', '--help'], 
                          capture_output=True, text=True, timeout=5)
    if result.returncode == 0:
        print("   ‚úÖ llama-cli encontrado")
        print("   üí° Para usar GPU: --n-gpu-layers 35 (ou m√°ximo)")
    else:
        print("   ‚ö†Ô∏è  llama-cli n√£o encontrado")
except:
    print("   ‚ö†Ô∏è  llama-cli n√£o encontrado")
    print("   üí° Qwen provavelmente usa llama.cpp via FFI ou biblioteca Rust")

print()

# 3. Verificar configura√ß√µes
print("3Ô∏è‚É£ Configura√ß√µes:")
env_vars = {
    "VRPG_ASR_USE_GPU": os.getenv("VRPG_ASR_USE_GPU", "n√£o definida"),
    "VRPG_LLM_USE_GPU": os.getenv("VRPG_LLM_USE_GPU", "n√£o definida"),
    "VRPG_GPU_LAYERS": os.getenv("VRPG_GPU_LAYERS", "n√£o definida"),
}

for var, value in env_vars.items():
    status = "‚úÖ" if value.lower() in ["true", "35"] else "‚ö†Ô∏è"
    print(f"   {status} {var} = {value}")

print()

# 4. Resumo
print("=" * 70)
print("üìä Resumo:")
print("=" * 70)
print()
print("Whisper:")
print("   - faster-whisper: Suporta GPU (device='cuda')")
print("   - whisper.cpp: Suporta GPU se compilado com CUDA")
print("   - Status: Verificar implementa√ß√£o no c√≥digo Rust")
print()
print("Qwen:")
print("   - llama.cpp: Suporta GPU via --n-gpu-layers")
print("   - Status: Verificar se est√° usando GPU layers no c√≥digo")
print()
print("üí° Para garantir uso de GPU:")
print("   1. Configure vari√°veis de ambiente (.env)")
print("   2. Verifique implementa√ß√£o nos arquivos Rust:")
print("      - src/asr-service/src/whisper.rs")
print("      - src/llm-core/src/inference.rs")
print("=" * 70)

