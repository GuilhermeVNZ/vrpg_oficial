#!/usr/bin/env python3
"""Verifica status GPU de todos os servi√ßos"""

import sys
import os

print("=" * 70)
print("üîç Status GPU - Todos os Servi√ßos")
print("=" * 70)
print()

# 1. PyTorch/CUDA
print("1Ô∏è‚É£ PyTorch/CUDA (Base):")
try:
    import torch
    print(f"   ‚úÖ PyTorch: {torch.__version__}")
    print(f"   ‚úÖ CUDA: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   ‚úÖ GPU: {torch.cuda.get_device_name(0)}")
    print()
except ImportError:
    print("   ‚ùå PyTorch n√£o instalado")
    print()

# 2. XTTS
print("2Ô∏è‚É£ XTTS (TTS):")
try:
    from TTS.api import TTS
    import torch
    tts = TTS('tts_models/multilingual/multi-dataset/xtts_v2', gpu=torch.cuda.is_available())
    print(f"   ‚úÖ XTTS: Funcionando")
    print(f"   ‚úÖ GPU: {torch.cuda.is_available()}")
    print()
except Exception as e:
    print(f"   ‚ö†Ô∏è  XTTS: {e}")
    print()

# 3. SoVITS
print("3Ô∏è‚É£ SoVITS:")
sovits_venv = "assets-and-models/models/tts/sovits/venv310/Scripts/python.exe"
if os.path.exists(sovits_venv):
    try:
        import subprocess
        result = subprocess.run([sovits_venv, '-c', 'import torch; print(f"CUDA: {torch.cuda.is_available()}")'], 
                               capture_output=True, text=True, timeout=5)
        if result.returncode == 0:
            print(f"   ‚úÖ SoVITS venv: OK")
            print(f"   {result.stdout.strip()}")
        else:
            print(f"   ‚ö†Ô∏è  SoVITS venv: Erro")
    except:
        print(f"   ‚ö†Ô∏è  SoVITS venv: N√£o verificado")
else:
    print(f"   ‚ö†Ô∏è  SoVITS venv: N√£o encontrado")
print()

# 4. Whisper
print("4Ô∏è‚É£ Whisper (ASR):")
try:
    from faster_whisper import WhisperModel
    import torch
    print(f"   ‚úÖ faster-whisper: Instalado")
    print(f"   ‚úÖ GPU dispon√≠vel: {torch.cuda.is_available()}")
    print(f"   ‚ö†Ô∏è  Status: STUB no c√≥digo Rust (n√£o usa modelo real)")
    print(f"   üí° Para usar GPU: Implementar Python bridge (como XTTS)")
except ImportError:
    print(f"   ‚ö†Ô∏è  faster-whisper: N√£o instalado")
    print(f"   üí° Instalar: pip install faster-whisper")
    print(f"   ‚ö†Ô∏è  Status: STUB no c√≥digo Rust (n√£o usa modelo real)")
print()

# 5. Qwen
print("5Ô∏è‚É£ Qwen (LLM):")
try:
    import subprocess
    result = subprocess.run(['llama-cli', '--help'], 
                          capture_output=True, text=True, timeout=5)
    if result.returncode == 0:
        print(f"   ‚úÖ llama.cpp: Encontrado")
        print(f"   üí° Para usar GPU: --n-gpu-layers 35")
    else:
        print(f"   ‚ö†Ô∏è  llama.cpp: N√£o encontrado")
except:
    print(f"   ‚ö†Ô∏è  llama.cpp: N√£o encontrado")
print(f"   ‚ö†Ô∏è  Status: STUB no c√≥digo Rust (n√£o usa modelo real)")
print(f"   üí° Para usar GPU: Integrar llama.cpp com GPU layers")
print()

# 6. Vari√°veis de Ambiente
print("6Ô∏è‚É£ Vari√°veis de Ambiente:")
env_vars = {
    "VRPG_GPU_ENABLED": os.getenv("VRPG_GPU_ENABLED", "n√£o definida"),
    "VRPG_TTS_USE_GPU": os.getenv("VRPG_TTS_USE_GPU", "n√£o definida"),
    "VRPG_ASR_USE_GPU": os.getenv("VRPG_ASR_USE_GPU", "n√£o definida"),
    "VRPG_LLM_USE_GPU": os.getenv("VRPG_LLM_USE_GPU", "n√£o definida"),
    "VRPG_SOVITS_USE_GPU": os.getenv("VRPG_SOVITS_USE_GPU", "n√£o definida"),
    "VRPG_GPU_LAYERS": os.getenv("VRPG_GPU_LAYERS", "n√£o definida"),
}

for var, value in env_vars.items():
    status = "‚úÖ" if value.lower() in ["true", "35"] else "‚ö†Ô∏è"
    print(f"   {status} {var} = {value}")

print()
print("=" * 70)
print("üìä RESUMO")
print("=" * 70)
print()
print("‚úÖ Funcionando com GPU:")
print("   - PyTorch/CUDA: ‚úÖ")
print("   - XTTS: ‚úÖ")
print("   - SoVITS: ‚úÖ (auto-detecta GPU)")
print()
print("‚ö†Ô∏è  Ainda n√£o implementado (STUBs):")
print("   - Whisper: ‚ö†Ô∏è  Precisa implementa√ß√£o real")
print("   - Qwen: ‚ö†Ô∏è  Precisa implementa√ß√£o real")
print()
print("üí° Pr√≥ximos passos:")
print("   1. Implementar Whisper com faster-whisper + GPU")
print("   2. Implementar Qwen com llama.cpp + GPU layers")
print("   3. Configurar vari√°veis de ambiente no .env")
print("=" * 70)

